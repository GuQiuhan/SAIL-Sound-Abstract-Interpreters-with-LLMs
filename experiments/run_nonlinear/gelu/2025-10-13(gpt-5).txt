====================QUERY========================
You are a formal methods expert working on neural network verification.
Your task is to generate the DeepPoly transformers for DNN operators.
Generate the transformer in Constraintflow DSL.

Here is the grammar of Constraintflow DSL:

'''
expr_list : expr COMMA expr_list
    |   expr ;

exprs: expr exprs
    | expr;

metadata: WEIGHT
    |   BIAS
    |   EQUATIONS
    |   LAYER ;

expr: FALSE                                         #false
    | TRUE                                          #true
    | IntConst                                      #int
    | FloatConst                                    #float
    | VAR                                           #varExp
    | EPSILON                                       #epsilon
    | CURR                                          #curr
    | PREV                                          #prev
    | PREV_0                                        #prev_0
    | PREV_1                                        #prev_1
    | CURRLIST                                      #curr_list
    | LPAREN expr RPAREN                            #parenExp
    | LSQR expr_list RSQR                           #exprarray
    | expr LSQR metadata RSQR                       #getMetadata
    | expr LSQR VAR RSQR                            #getElement
    | expr binop expr                               #binopExp
    | NOT expr                                      #not
    | MINUS expr                                    #neg
    | expr QUES expr COLON expr                     #cond
    | expr DOT TRAV LPAREN direction COMMA expr COMMA expr COMMA expr RPAREN LBRACE expr RBRACE     #traverse
    | argmax_op LPAREN expr COMMA expr RPAREN       #argmaxOp
    | max_op LPAREN expr RPAREN                     #maxOpList
    | max_op LPAREN expr COMMA expr RPAREN          #maxOp
    | list_op LPAREN expr RPAREN                    #listOp
    | expr DOT MAP LPAREN expr RPAREN               #map
    | expr DOT MAPLIST LPAREN expr RPAREN           #map_list
    | expr DOT DOTT LPAREN expr RPAREN              #dot
    | expr DOT CONCAT LPAREN expr RPAREN            #concat
    | LP LPAREN lp_op COMMA expr COMMA expr RPAREN  #lp
    | VAR LPAREN expr_list RPAREN                   #funcCall
    | VAR exprs                                     #curry
;

trans_ret :
    expr QUES trans_ret COLON trans_ret #condtrans
    | LPAREN trans_ret RPAREN #parentrans
    | expr_list #trans
;
'''

DeepPoly certifier uses four kinds of bounds to approximate the operator: (Float l, Float u, PolyExp L, PolyExp U).
They must follow the constraints that: curr[l] <= curr <= curr[u] & curr[L] <= curr <= curr[U]. `curr` here means the current neuron, `prev` means the inputs to the operator.
When the operator takes multiple inputs, use `prev_0`, `prev_1`, ... to refer to each input.
So every transformer in each case of the case analysis must return four values. Use any funstions below if needed instead of use arithmetic operators.
Function you can use:
- func simplify_lower(Neuron n, Float coeff) = (coeff >= 0) ? (coeff * n[l]) : (coeff * n[u]);
- func simplify_upper(Neuron n, Float coeff) = (coeff >= 0) ? (coeff * n[u]) : (coeff * n[l]);
- func replace_lower(Neuron n, Float coeff) = (coeff >= 0) ? (coeff * n[L]) : (coeff * n[U]);
- func replace_upper(Neuron n, Float coeff) = (coeff >= 0) ? (coeff * n[U]) : (coeff * n[L]);
- func priority(Neuron n) = n[layer];
- func priority2(Neuron n, Float c) = -n[layer];
- func stop(Neuron n) = false;
- func stop_traverse(Neuron n, Float c) = false;
- func backsubs_lower(PolyExp e, Neuron n) = (e.traverse(backward, priority2, stop_traverse, replace_lower){e <= n}).map(simplify_lower);
- func backsubs_upper(PolyExp e, Neuron n) = (e.traverse(backward, priority2, stop_traverse, replace_upper){e >= n}).map(simplify_upper);
- func f(Neuron n1, Neuron n2) = n1[l] >= n2[u];
- func slope(Float x1, Float x2) = ((x1 * (x1 + 3))-(x2 * (x2 + 3))) / (6 * (x1-x2));
- func intercept(Float x1, Float x2) = x1 * ((x1 + 3) / 6) - (slope(x1, x2) * x1);
- func f(Neuron n1, Neuron n2) = n1[l] >= n2[u];
- func f1(Float x) = x < 3 ? x * ((x + 3) / 6) : x;
- func f2(Float x) = x * ((x + 3) / 6);
- func f3(Neuron n) = max(f2(n[l]), f2(n[u]));
- func compute_l(Neuron n1, Neuron n2) = min([n1[l]*n2[l], n1[l]*n2[u], n1[u]*n2[l], n1[u]*n2[u]]);
- func compute_u(Neuron n1, Neuron n2) = max([n1[l]*n2[l], n1[l]*n2[u], n1[u]*n2[l], n1[u]*n2[u]]);
- func avg(List<Float> xs) = sum(xs) / len(xs);
- func argmax(List<Neuron> ns, (Neuron, Neuron -> Bool) cmp) = [ n | n in ns, forall m in ns. cmp(n, m) ];
- func argmin(List<Neuron> ns, (Neuron, Neuron -> Bool) cmp) = [ n | n in ns, forall m in ns. cmp(n, m) ];
- func sigma(Float x) = 1/(1+eps(-x));

Don't add comments to DSL.

### Example: ReLU operator
Input: Generate the transformer for `relu` operator
Output:
def Shape as (Float l, Float u, PolyExp L, PolyExp U){[(curr[l]<=curr),(curr[u]>=curr),(curr[L]<=curr),(curr[U]>=curr)]};

transformer deeppoly{
    Relu -> ((prev[l]) >= 0) ? ((prev[l]), (prev[u]), (prev), (prev)) : (((prev[u]) <= 0) ? (0, 0, 0, 0) : (0, (prev[u]), 0, (((prev[u]) / ((prev[u]) - (prev[l]))) * (prev)) - (((prev[u]) * (prev[l])) / ((prev[u]) - (prev[l]))) ));
}

### Example: Abs operator
Input: Generate the transformer for `abs` operator
Output:
def Shape as (Float l, Float u, PolyExp L, PolyExp U){[(curr[l]<=curr),(curr[u]>=curr),(curr[L]<=curr),(curr[U]>=curr)]};

transformer deeppoly{
    Abs -> ((prev[l]) >= 0) ? ((prev[l]), (prev[u]), (prev), (prev)) : (((prev[u]) <= 0) ? (0-(prev[u]), 0-(prev[l]), 0-(prev), 0-(prev)) : (0, max(prev[u], 0-prev[l]), prev, prev*(prev[u]+prev[l])/(prev[u]-prev[l]) - (((2*prev[u])*prev[l])/(prev[u]-prev[l]))) );
}

### Example: Affine operator
Input: Generate the transformer for `affine` operator
Output:
def Shape as (Float l, Float u, PolyExp L, PolyExp U){[(curr[l]<=curr),(curr[u]>=curr),(curr[L]<=curr),(curr[U]>=curr)]};

transformer deeppoly{
    Affine -> (backsubs_lower(prev.dot(curr[weight]) + curr[bias], curr), backsubs_upper(prev.dot(curr[weight]) + curr[bias], curr), prev.dot(curr[weight]) + curr[bias], prev.dot(curr[weight]) + curr[bias]);
}

### Now generate the transformer for `Gelu` operator
Input: Generate the transformer for `Gelu` operator
Output:

====================REPLY========================
def Shape as (Float l, Float u, PolyExp L, PolyExp U){[(curr[l]<=curr),(curr[u]>=curr),(curr[L]<=curr),(curr[U]>=curr)]};

transformer deeppoly{
    Gelu -> (0.5*(prev[l])*(1 + erf((prev[l]) / 1.4142135623730951)), 0.5*(prev[u])*(1 + erf((prev[u]) / 1.4142135623730951)), 0.5*(prev[l])*(1 + erf((prev[l]) / 1.4142135623730951)), 0.5*(prev[u])*(1 + erf((prev[u]) / 1.4142135623730951)));
}

====================QUERY========================
You are a formal methods expert working on neural network verification.
Your task is to generate the DeepPoly transformers for DNN operators.
Generate the transformer in Constraintflow DSL.

Here is the grammar of Constraintflow DSL:

'''
expr_list : expr COMMA expr_list
    |   expr ;

exprs: expr exprs
    | expr;

metadata: WEIGHT
    |   BIAS
    |   EQUATIONS
    |   LAYER ;

expr: FALSE                                         #false
    | TRUE                                          #true
    | IntConst                                      #int
    | FloatConst                                    #float
    | VAR                                           #varExp
    | EPSILON                                       #epsilon
    | CURR                                          #curr
    | PREV                                          #prev
    | PREV_0                                        #prev_0
    | PREV_1                                        #prev_1
    | CURRLIST                                      #curr_list
    | LPAREN expr RPAREN                            #parenExp
    | LSQR expr_list RSQR                           #exprarray
    | expr LSQR metadata RSQR                       #getMetadata
    | expr LSQR VAR RSQR                            #getElement
    | expr binop expr                               #binopExp
    | NOT expr                                      #not
    | MINUS expr                                    #neg
    | expr QUES expr COLON expr                     #cond
    | expr DOT TRAV LPAREN direction COMMA expr COMMA expr COMMA expr RPAREN LBRACE expr RBRACE     #traverse
    | argmax_op LPAREN expr COMMA expr RPAREN       #argmaxOp
    | max_op LPAREN expr RPAREN                     #maxOpList
    | max_op LPAREN expr COMMA expr RPAREN          #maxOp
    | list_op LPAREN expr RPAREN                    #listOp
    | expr DOT MAP LPAREN expr RPAREN               #map
    | expr DOT MAPLIST LPAREN expr RPAREN           #map_list
    | expr DOT DOTT LPAREN expr RPAREN              #dot
    | expr DOT CONCAT LPAREN expr RPAREN            #concat
    | LP LPAREN lp_op COMMA expr COMMA expr RPAREN  #lp
    | VAR LPAREN expr_list RPAREN                   #funcCall
    | VAR exprs                                     #curry
;

trans_ret :
    expr QUES trans_ret COLON trans_ret #condtrans
    | LPAREN trans_ret RPAREN #parentrans
    | expr_list #trans
;
'''

DeepPoly certifier uses four kinds of bounds to approximate the operator: (Float l, Float u, PolyExp L, PolyExp U).
They must follow the constraints that: curr[l] <= curr <= curr[u] & curr[L] <= curr <= curr[U]. `curr` here means the current neuron, `prev` means the inputs to the operator.
When the operator takes multiple inputs, use `prev_0`, `prev_1`, ... to refer to each input.
So every transformer in each case of the case analysis must return four values. Use any funstions below if needed instead of use arithmetic operators.
Function you can use:
- func simplify_lower(Neuron n, Float coeff) = (coeff >= 0) ? (coeff * n[l]) : (coeff * n[u]);
- func simplify_upper(Neuron n, Float coeff) = (coeff >= 0) ? (coeff * n[u]) : (coeff * n[l]);
- func replace_lower(Neuron n, Float coeff) = (coeff >= 0) ? (coeff * n[L]) : (coeff * n[U]);
- func replace_upper(Neuron n, Float coeff) = (coeff >= 0) ? (coeff * n[U]) : (coeff * n[L]);
- func priority(Neuron n) = n[layer];
- func priority2(Neuron n, Float c) = -n[layer];
- func stop(Neuron n) = false;
- func stop_traverse(Neuron n, Float c) = false;
- func backsubs_lower(PolyExp e, Neuron n) = (e.traverse(backward, priority2, stop_traverse, replace_lower){e <= n}).map(simplify_lower);
- func backsubs_upper(PolyExp e, Neuron n) = (e.traverse(backward, priority2, stop_traverse, replace_upper){e >= n}).map(simplify_upper);
- func f(Neuron n1, Neuron n2) = n1[l] >= n2[u];
- func slope(Float x1, Float x2) = ((x1 * (x1 + 3))-(x2 * (x2 + 3))) / (6 * (x1-x2));
- func intercept(Float x1, Float x2) = x1 * ((x1 + 3) / 6) - (slope(x1, x2) * x1);
- func f(Neuron n1, Neuron n2) = n1[l] >= n2[u];
- func f1(Float x) = x < 3 ? x * ((x + 3) / 6) : x;
- func f2(Float x) = x * ((x + 3) / 6);
- func f3(Neuron n) = max(f2(n[l]), f2(n[u]));
- func compute_l(Neuron n1, Neuron n2) = min([n1[l]*n2[l], n1[l]*n2[u], n1[u]*n2[l], n1[u]*n2[u]]);
- func compute_u(Neuron n1, Neuron n2) = max([n1[l]*n2[l], n1[l]*n2[u], n1[u]*n2[l], n1[u]*n2[u]]);
- func avg(List<Float> xs) = sum(xs) / len(xs);
- func argmax(List<Neuron> ns, (Neuron, Neuron -> Bool) cmp) = [ n | n in ns, forall m in ns. cmp(n, m) ];
- func argmin(List<Neuron> ns, (Neuron, Neuron -> Bool) cmp) = [ n | n in ns, forall m in ns. cmp(n, m) ];
- func sigma(Float x) = 1/(1+eps(-x));

Don't add comments to DSL.

### Example: ReLU operator
Input: Generate the transformer for `relu` operator
Output:
def Shape as (Float l, Float u, PolyExp L, PolyExp U){[(curr[l]<=curr),(curr[u]>=curr),(curr[L]<=curr),(curr[U]>=curr)]};

transformer deeppoly{
    Relu -> ((prev[l]) >= 0) ? ((prev[l]), (prev[u]), (prev), (prev)) : (((prev[u]) <= 0) ? (0, 0, 0, 0) : (0, (prev[u]), 0, (((prev[u]) / ((prev[u]) - (prev[l]))) * (prev)) - (((prev[u]) * (prev[l])) / ((prev[u]) - (prev[l]))) ));
}

### Example: Abs operator
Input: Generate the transformer for `abs` operator
Output:
def Shape as (Float l, Float u, PolyExp L, PolyExp U){[(curr[l]<=curr),(curr[u]>=curr),(curr[L]<=curr),(curr[U]>=curr)]};

transformer deeppoly{
    Abs -> ((prev[l]) >= 0) ? ((prev[l]), (prev[u]), (prev), (prev)) : (((prev[u]) <= 0) ? (0-(prev[u]), 0-(prev[l]), 0-(prev), 0-(prev)) : (0, max(prev[u], 0-prev[l]), prev, prev*(prev[u]+prev[l])/(prev[u]-prev[l]) - (((2*prev[u])*prev[l])/(prev[u]-prev[l]))) );
}

### Example: Affine operator
Input: Generate the transformer for `affine` operator
Output:
def Shape as (Float l, Float u, PolyExp L, PolyExp U){[(curr[l]<=curr),(curr[u]>=curr),(curr[L]<=curr),(curr[U]>=curr)]};

transformer deeppoly{
    Affine -> (backsubs_lower(prev.dot(curr[weight]) + curr[bias], curr), backsubs_upper(prev.dot(curr[weight]) + curr[bias], curr), prev.dot(curr[weight]) + curr[bias], prev.dot(curr[weight]) + curr[bias]);
}

### Now generate the transformer for `Gelu` operator
Input: Generate the transformer for `Gelu` operator
Output:

# Previously generated (invalid) code:

def Shape as (Float l, Float u, PolyExp L, PolyExp U){[(curr[l]<=curr),(curr[u]>=curr),(curr[L]<=curr),(curr[U]>=curr)]};

transformer deeppoly{
    Gelu -> (0.5*(prev[l])*(1 + erf((prev[l]) / 1.4142135623730951)), 0.5*(prev[u])*(1 + erf((prev[u]) / 1.4142135623730951)), 0.5*(prev[l])*(1 + erf((prev[l]) / 1.4142135623730951)), 0.5*(prev[u])*(1 + erf((prev[u]) / 1.4142135623730951)));
}

# Counter Example respectively: [prev_l,prev_u]=[-2,-0.2]
# Learn from the failed generation above and revise your output accordingly. Output the DSL only.

====================REPLY========================
def Shape as (Float l, Float u, PolyExp L, PolyExp U){[(curr[l]<=curr),(curr[u]>=curr),(curr[L]<=curr),(curr[U]>=curr)]};

transformer deeppoly{
    Gelu -> (
        (((((0.5)*(prev[u])*((1)+(erf((prev[u])/(1.4142135623730951)))) ) <= ((0.5)*(prev[l])*((1)+(erf((prev[l])/(1.4142135623730951)))) )) ? ((0.5)*(prev[u])*((1)+(erf((prev[u])/(1.4142135623730951)))) ) : ((0.5)*(prev[l])*((1)+(erf((prev[l])/(1.4142135623730951)))) )) <= (-0.2)) ? ((((0.5)*(prev[u])*((1)+(erf((prev[u])/(1.4142135623730951)))) ) <= ((0.5)*(prev[l])*((1)+(erf((prev[l])/(1.4142135623730951)))) )) ? ((0.5)*(prev[u])*((1)+(erf((prev[u])/(1.4142135623730951)))) ) : ((0.5)*(prev[l])*((1)+(erf((prev[l])/(1.4142135623730951)))) ))) : (-0.2),
        ((((0.5)*(prev[u])*((1)+(erf((prev[u])/(1.4142135623730951)))) ) >= ((0.5)*(prev[l])*((1)+(erf((prev[l])/(1.4142135623730951)))) )) ? ((0.5)*(prev[u])*((1)+(erf((prev[u])/(1.4142135623730951)))) ) : ((0.5)*(prev[l])*((1)+(erf((prev[l])/(1.4142135623730951)))) )),
        (((((0.5)*(prev[u])*((1)+(erf((prev[u])/(1.4142135623730951)))) ) <= ((0.5)*(prev[l])*((1)+(erf((prev[l])/(1.4142135623730951)))) )) ? ((0.5)*(prev[u])*((1)+(erf((prev[u])/(1.4142135623730951)))) ) : ((0.5)*(prev[l])*((1)+(erf((prev[l])/(1.4142135623730951)))) )) <= (-0.2)) ? ((((0.5)*(prev[u])*((1)+(erf((prev[u])/(1.4142135623730951)))) ) <= ((0.5)*(prev[l])*((1)+(erf((prev[l])/(1.4142135623730951)))) )) ? ((0.5)*(prev[u])*((1)+(erf((prev[u])/(1.4142135623730951)))) ) : ((0.5)*(prev[l])*((1)+(erf((prev[l])/(1.4142135623730951)))) ))) : (-0.2),
        ((((0.5)*(prev[u])*((1)+(erf((prev[u])/(1.4142135623730951)))) ) >= ((0.5)*(prev[l])*((1)+(erf((prev[l])/(1.4142135623730951)))) )) ? ((0.5)*(prev[u])*((1)+(erf((prev[u])/(1.4142135623730951)))) ) : ((0.5)*(prev[l])*((1)+(erf((prev[l])/(1.4142135623730951)))) ))
    );
}


====================QUERY========================
You are a formal methods expert working on neural network verification.
Your task is to generate the DeepPoly transformers for DNN operators.
Generate the transformer in Constraintflow DSL.

Here is the grammar of Constraintflow DSL:

'''
expr_list : expr COMMA expr_list
    |   expr ;

exprs: expr exprs
    | expr;

metadata: WEIGHT
    |   BIAS
    |   EQUATIONS
    |   LAYER ;

expr: FALSE                                         #false
    | TRUE                                          #true
    | IntConst                                      #int
    | FloatConst                                    #float
    | VAR                                           #varExp
    | EPSILON                                       #epsilon
    | CURR                                          #curr
    | PREV                                          #prev
    | PREV_0                                        #prev_0
    | PREV_1                                        #prev_1
    | CURRLIST                                      #curr_list
    | LPAREN expr RPAREN                            #parenExp
    | LSQR expr_list RSQR                           #exprarray
    | expr LSQR metadata RSQR                       #getMetadata
    | expr LSQR VAR RSQR                            #getElement
    | expr binop expr                               #binopExp
    | NOT expr                                      #not
    | MINUS expr                                    #neg
    | expr QUES expr COLON expr                     #cond
    | expr DOT TRAV LPAREN direction COMMA expr COMMA expr COMMA expr RPAREN LBRACE expr RBRACE     #traverse
    | argmax_op LPAREN expr COMMA expr RPAREN       #argmaxOp
    | max_op LPAREN expr RPAREN                     #maxOpList
    | max_op LPAREN expr COMMA expr RPAREN          #maxOp
    | list_op LPAREN expr RPAREN                    #listOp
    | expr DOT MAP LPAREN expr RPAREN               #map
    | expr DOT MAPLIST LPAREN expr RPAREN           #map_list
    | expr DOT DOTT LPAREN expr RPAREN              #dot
    | expr DOT CONCAT LPAREN expr RPAREN            #concat
    | LP LPAREN lp_op COMMA expr COMMA expr RPAREN  #lp
    | VAR LPAREN expr_list RPAREN                   #funcCall
    | VAR exprs                                     #curry
;

trans_ret :
    expr QUES trans_ret COLON trans_ret #condtrans
    | LPAREN trans_ret RPAREN #parentrans
    | expr_list #trans
;
'''

DeepPoly certifier uses four kinds of bounds to approximate the operator: (Float l, Float u, PolyExp L, PolyExp U).
They must follow the constraints that: curr[l] <= curr <= curr[u] & curr[L] <= curr <= curr[U]. `curr` here means the current neuron, `prev` means the inputs to the operator.
When the operator takes multiple inputs, use `prev_0`, `prev_1`, ... to refer to each input.
So every transformer in each case of the case analysis must return four values. Use any funstions below if needed instead of use arithmetic operators.
Function you can use:
- func simplify_lower(Neuron n, Float coeff) = (coeff >= 0) ? (coeff * n[l]) : (coeff * n[u]);
- func simplify_upper(Neuron n, Float coeff) = (coeff >= 0) ? (coeff * n[u]) : (coeff * n[l]);
- func replace_lower(Neuron n, Float coeff) = (coeff >= 0) ? (coeff * n[L]) : (coeff * n[U]);
- func replace_upper(Neuron n, Float coeff) = (coeff >= 0) ? (coeff * n[U]) : (coeff * n[L]);
- func priority(Neuron n) = n[layer];
- func priority2(Neuron n, Float c) = -n[layer];
- func stop(Neuron n) = false;
- func stop_traverse(Neuron n, Float c) = false;
- func backsubs_lower(PolyExp e, Neuron n) = (e.traverse(backward, priority2, stop_traverse, replace_lower){e <= n}).map(simplify_lower);
- func backsubs_upper(PolyExp e, Neuron n) = (e.traverse(backward, priority2, stop_traverse, replace_upper){e >= n}).map(simplify_upper);
- func f(Neuron n1, Neuron n2) = n1[l] >= n2[u];
- func slope(Float x1, Float x2) = ((x1 * (x1 + 3))-(x2 * (x2 + 3))) / (6 * (x1-x2));
- func intercept(Float x1, Float x2) = x1 * ((x1 + 3) / 6) - (slope(x1, x2) * x1);
- func f(Neuron n1, Neuron n2) = n1[l] >= n2[u];
- func f1(Float x) = x < 3 ? x * ((x + 3) / 6) : x;
- func f2(Float x) = x * ((x + 3) / 6);
- func f3(Neuron n) = max(f2(n[l]), f2(n[u]));
- func compute_l(Neuron n1, Neuron n2) = min([n1[l]*n2[l], n1[l]*n2[u], n1[u]*n2[l], n1[u]*n2[u]]);
- func compute_u(Neuron n1, Neuron n2) = max([n1[l]*n2[l], n1[l]*n2[u], n1[u]*n2[l], n1[u]*n2[u]]);
- func avg(List<Float> xs) = sum(xs) / len(xs);
- func argmax(List<Neuron> ns, (Neuron, Neuron -> Bool) cmp) = [ n | n in ns, forall m in ns. cmp(n, m) ];
- func argmin(List<Neuron> ns, (Neuron, Neuron -> Bool) cmp) = [ n | n in ns, forall m in ns. cmp(n, m) ];
- func sigma(Float x) = 1/(1+eps(-x));

Don't add comments to DSL.

### Example: ReLU operator
Input: Generate the transformer for `relu` operator
Output:
def Shape as (Float l, Float u, PolyExp L, PolyExp U){[(curr[l]<=curr),(curr[u]>=curr),(curr[L]<=curr),(curr[U]>=curr)]};

transformer deeppoly{
    Relu -> ((prev[l]) >= 0) ? ((prev[l]), (prev[u]), (prev), (prev)) : (((prev[u]) <= 0) ? (0, 0, 0, 0) : (0, (prev[u]), 0, (((prev[u]) / ((prev[u]) - (prev[l]))) * (prev)) - (((prev[u]) * (prev[l])) / ((prev[u]) - (prev[l]))) ));
}

### Example: Abs operator
Input: Generate the transformer for `abs` operator
Output:
def Shape as (Float l, Float u, PolyExp L, PolyExp U){[(curr[l]<=curr),(curr[u]>=curr),(curr[L]<=curr),(curr[U]>=curr)]};

transformer deeppoly{
    Abs -> ((prev[l]) >= 0) ? ((prev[l]), (prev[u]), (prev), (prev)) : (((prev[u]) <= 0) ? (0-(prev[u]), 0-(prev[l]), 0-(prev), 0-(prev)) : (0, max(prev[u], 0-prev[l]), prev, prev*(prev[u]+prev[l])/(prev[u]-prev[l]) - (((2*prev[u])*prev[l])/(prev[u]-prev[l]))) );
}

### Example: Affine operator
Input: Generate the transformer for `affine` operator
Output:
def Shape as (Float l, Float u, PolyExp L, PolyExp U){[(curr[l]<=curr),(curr[u]>=curr),(curr[L]<=curr),(curr[U]>=curr)]};

transformer deeppoly{
    Affine -> (backsubs_lower(prev.dot(curr[weight]) + curr[bias], curr), backsubs_upper(prev.dot(curr[weight]) + curr[bias], curr), prev.dot(curr[weight]) + curr[bias], prev.dot(curr[weight]) + curr[bias]);
}

### Now generate the transformer for `Gelu` operator
Input: Generate the transformer for `Gelu` operator
Output:

# Previously generated (imprecise) code:

def Shape as (Float l, Float u, PolyExp L, PolyExp U){[(curr[l]<=curr),(curr[u]>=curr),(curr[L]<=curr),(curr[U]>=curr)]};

transformer deeppoly{
    Gelu -> (
        (((((0.5)*(prev[u])*((1)+(erf((prev[u])/(1.4142135623730951)))) ) <= ((0.5)*(prev[l])*((1)+(erf((prev[l])/(1.4142135623730951)))) )) ? ((0.5)*(prev[u])*((1)+(erf((prev[u])/(1.4142135623730951)))) ) : ((0.5)*(prev[l])*((1)+(erf((prev[l])/(1.4142135623730951)))) )) <= (-0.2)) ? ((((0.5)*(prev[u])*((1)+(erf((prev[u])/(1.4142135623730951)))) ) <= ((0.5)*(prev[l])*((1)+(erf((prev[l])/(1.4142135623730951)))) )) ? ((0.5)*(prev[u])*((1)+(erf((prev[u])/(1.4142135623730951)))) ) : ((0.5)*(prev[l])*((1)+(erf((prev[l])/(1.4142135623730951)))) ))) : (-0.2),
        ((((0.5)*(prev[u])*((1)+(erf((prev[u])/(1.4142135623730951)))) ) >= ((0.5)*(prev[l])*((1)+(erf((prev[l])/(1.4142135623730951)))) )) ? ((0.5)*(prev[u])*((1)+(erf((prev[u])/(1.4142135623730951)))) ) : ((0.5)*(prev[l])*((1)+(erf((prev[l])/(1.4142135623730951)))) )),
        (((((0.5)*(prev[u])*((1)+(erf((prev[u])/(1.4142135623730951)))) ) <= ((0.5)*(prev[l])*((1)+(erf((prev[l])/(1.4142135623730951)))) )) ? ((0.5)*(prev[u])*((1)+(erf((prev[u])/(1.4142135623730951)))) ) : ((0.5)*(prev[l])*((1)+(erf((prev[l])/(1.4142135623730951)))) )) <= (-0.2)) ? ((((0.5)*(prev[u])*((1)+(erf((prev[u])/(1.4142135623730951)))) ) <= ((0.5)*(prev[l])*((1)+(erf((prev[l])/(1.4142135623730951)))) )) ? ((0.5)*(prev[u])*((1)+(erf((prev[u])/(1.4142135623730951)))) ) : ((0.5)*(prev[l])*((1)+(erf((prev[l])/(1.4142135623730951)))) ))) : (-0.2),
        ((((0.5)*(prev[u])*((1)+(erf((prev[u])/(1.4142135623730951)))) ) >= ((0.5)*(prev[l])*((1)+(erf((prev[l])/(1.4142135623730951)))) )) ? ((0.5)*(prev[u])*((1)+(erf((prev[u])/(1.4142135623730951)))) ) : ((0.5)*(prev[l])*((1)+(erf((prev[l])/(1.4142135623730951)))) ))
    );
}

# Learn from the imprecise generation above and revise your output accordingly. Note the the L and U in the deeppoly domain are supposed to be affine functions. Output the DSL only.

====================REPLY========================
// Shape: (l, u, L, U)
def Shape as (Float l, Float u, PolyExp L, PolyExp U){
  [(curr[l] <= curr), (curr[u] >= curr), (curr[L] <= curr), (curr[U] >= curr)]
};

// -------- Helper functions (constants inlined) --------
// Phi(x) = 0.5*(1 + erf(x / sqrt(2)))
def Phi as (Float x){ 0.5 * (1 + erf(x / 1.4142135623730951)) };
// phi(x) = (1/sqrt(2Ï€)) * exp(-x^2/2)
def phi as (Float x){ 0.3989422804014327 * exp(-0.5 * x * x) };
// GELU
def F as (Float x){ 0.5 * x * (1 + erf(x / 1.4142135623730951)) };
// GELU'
def dF as (Float x){ Phi(x) + x * phi(x) };

// Secant s(x) = a_s * x + b_s
def SecantA as (Float l, Float u){ (F(u) - F(l)) / (u - l) };
def SecantB as (Float l, Float u){ F(l) - SecantA(l,u) * l };

// Tangents at endpoints
def TangentA_l as (Float l){ dF(l) };
def TangentB_l as (Float l){ F(l) - TangentA_l(l) * l };
def TangentA_u as (Float u){ dF(u) };
def TangentB_u as (Float u){ F(u) - TangentA_u(u) * u };

// Midpoint and tangent values at midpoint
def Mid as (Float l, Float u){ (l + u) / 2.0 };
def T_l_at_m as (Float l, Float u){ TangentA_l(l) * Mid(l,u) + TangentB_l(l) };
def T_u_at_m as (Float l, Float u){ TangentA_u(u) * Mid(l,u) + TangentB_u(u) };

// -------- DeepPoly transformer for GELU --------
transformer deeppoly{
  Gelu -> (
    // ===== Interval output bounds (l_out, u_out) =====
    // l_out = min{ F(l), F(u), -0.18 if interval hits the minimum window [-0.9, -0.6] }
    // u_out = max{ F(l), F(u) }
    (
      // if intersects minimum window [-0.9, -0.6]
      ( (prev[l] <= -0.6) ? ( (prev[u] >= -0.9) ?
          // min( min(F(l),F(u)), -0.18 )
          ( ((F(prev[l]) <= F(prev[u])) ? F(prev[l]) : F(prev[u])) <= (-0.18)
              ? ((F(prev[l]) <= F(prev[u])) ? F(prev[l]) : F(prev[u]))
              : (-0.18) )
        :  // not intersect: min(F(l),F(u))
          ( (F(prev[l]) <= F(prev[u])) ? F(prev[l]) : F(prev[u]) ) )
        :  // not intersect: min(F(l),F(u))
          ( (F(prev[l]) <= F(prev[u])) ? F(prev[l]) : F(prev[u]) ) )
    ),
    ( (F(prev[l]) >= F(prev[u])) ? F(prev[l]) : F(prev[u]) ),

    // ===== Affine lower bound L(prev) =====
    // Case order without logical AND: left-concave, right-concave, convex, else mixed (constants)
    (
      // Case A: entirely concave on the left (u <= -sqrt(2))
      (prev[u] <= -1.4142135623730951) ?
        ( SecantA(prev[l],prev[u]) * prev + SecantB(prev[l],prev[u]) )
      :
      // Case B: entirely concave on the right (l >= +sqrt(2))
      (prev[l] >=  1.4142135623730951) ?
        ( SecantA(prev[l],prev[u]) * prev + SecantB(prev[l],prev[u]) )
      :
      // Case C: entirely convex (-sqrt(2) <= l and u <= +sqrt(2))
      ( (prev[l] >= -1.4142135623730951) ?
          ( (prev[u] <=  1.4142135623730951) ?
              // choose the larger tangent at midpoint for the lower bound in convex case
              ( ( T_l_at_m(prev[l],prev[u]) >= T_u_at_m(prev[l],prev[u]) )
                  ? ( TangentA_l(prev[l]) * prev + TangentB_l(prev[l]) )
                  : ( TangentA_u(prev[u]) * prev + TangentB_u(prev[u]) ) )
            :  // mixed
              // constant affine (alpha=0), safe but looser
              ( ((F(prev[l]) <= F(prev[u])) ? F(prev[l]) : F(prev[u])) <= (-0.18)
                  ? ((F(prev[l]) <= F(prev[u])) ? F(prev[l]) : F(prev[u]))
                  : (-0.18) )
        )
        :  // mixed
          ( ((F(prev[l]) <= F(prev[u])) ? F(prev[l]) : F(prev[u])) <= (-0.18)
              ? ((F(prev[l]) <= F(prev[u])) ? F(prev[l]) : F(prev[u]))
              : (-0.18) )
    ),

    // ===== Affine upper bound U(prev) =====
    (
      // Case A: entirely concave on the left (u <= -sqrt(2))
      (prev[u] <= -1.4142135623730951) ?
        // choose the smaller tangent at midpoint for the upper bound in concave case
        ( ( T_l_at_m(prev[l],prev[u]) <= T_u_at_m(prev[l],prev[u]) )
            ? ( TangentA_l(prev[l]) * prev + TangentB_l(prev[l]) )
            : ( TangentA_u(prev[u]) * prev + TangentB_u(prev[u]) ) )
      :
      // Case B: entirely concave on the right (l >= +sqrt(2))
      (prev[l] >=  1.4142135623730951) ?
        ( ( T_l_at_m(prev[l],prev[u]) <= T_u_at_m(prev[l],prev[u]) )
            ? ( TangentA_l(prev[l]) * prev + TangentB_l(prev[l]) )
            : ( TangentA_u(prev[u]) * prev + TangentB_u(prev[u]) ) )
      :
      // Case C: entirely convex (-sqrt(2) <= l and u <= +sqrt(2)) -> secant as upper bound
      ( (prev[l] >= -1.4142135623730951) ?
          ( (prev[u] <=  1.4142135623730951) ?
              ( SecantA(prev[l],prev[u]) * prev + SecantB(prev[l],prev[u]) )
            :  // mixed
              ( (F(prev[l]) >= F(prev[u])) ? F(prev[l]) : F(prev[u]) ) )
        :  // mixed
          ( (F(prev[l]) >= F(prev[u])) ? F(prev[l]) : F(prev[u]) ) )
    )
  );
}
